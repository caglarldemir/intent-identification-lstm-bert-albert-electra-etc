{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1119749,"sourceType":"datasetVersion","datasetId":628714}],"dockerImageVersionId":29908,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport os\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Activation, LSTM, SimpleRNN, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"TensorFlow Version:\",tf.__version__)\nprint(\"Hub version: \",hub.__version__)\n# Params for bert model and tokenization\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-07T14:18:57.523722Z","iopub.execute_input":"2024-04-07T14:18:57.524105Z","iopub.status.idle":"2024-04-07T14:18:57.534324Z","shell.execute_reply.started":"2024-04-07T14:18:57.524047Z","shell.execute_reply":"2024-04-07T14:18:57.533457Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"TensorFlow Version: 2.1.0\nHub version:  0.8.0\n","output_type":"stream"}]},{"cell_type":"code","source":"class LoadingData():\n            \n    def __init__(self):\n        train_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Train\")\n        validation_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Validate\")\n        category_id = 0\n        self.cat_to_intent = {}\n        self.intent_to_cat = {}\n        \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                self.cat_to_intent[category_id] = intent_id\n                self.intent_to_cat[intent_id] = category_id\n                category_id+=1\n        print(self.cat_to_intent)\n        print(self.intent_to_cat)\n        '''Training data'''\n        training_data = list() \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                training_data+=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])\n        self.train_data_frame = pd.DataFrame(training_data, columns =['query', 'intent','category'])   \n        \n        self.train_data_frame = self.train_data_frame.sample(frac = 1)\n\n\n        \n        '''Validation data'''\n        validation_data = list()    \n        for dirname, _, filenames in os.walk(validation_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                validation_data +=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])                \n        self.validation_data_frame = pd.DataFrame(validation_data, columns =['query', 'intent','category'])\n\n        self.validation_data_frame = self.validation_data_frame.sample(frac = 1)\n        \n        \n    def make_data_for_intent_from_json(self,json_file,intent_id,cat):\n        json_d = json.load(open(json_file))         \n        \n        json_dict = json_d[intent_id]\n\n        sent_list = list()\n        for i in json_dict:\n            each_list = i['data']\n            sent =\"\"\n            for i in each_list:\n                sent = sent + i['text']+ \" \"\n            sent =sent[:-1]\n            for i in range(3):\n                sent = sent.replace(\"  \",\" \")\n            sent_list.append((sent,intent_id,cat))\n        return sent_list\n            ","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2024-04-07T14:18:58.076082Z","iopub.execute_input":"2024-04-07T14:18:58.076421Z","iopub.status.idle":"2024-04-07T14:18:58.098007Z","shell.execute_reply.started":"2024-04-07T14:18:58.076378Z","shell.execute_reply":"2024-04-07T14:18:58.096974Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"load_data_obj = LoadingData()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:18:58.304934Z","iopub.execute_input":"2024-04-07T14:18:58.305248Z","iopub.status.idle":"2024-04-07T14:18:58.483542Z","shell.execute_reply.started":"2024-04-07T14:18:58.305213Z","shell.execute_reply":"2024-04-07T14:18:58.482688Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"{0: 'BookRestaurant', 1: 'SearchScreeningEvent', 2: 'RateBook', 3: 'GetWeather', 4: 'AddToPlaylist', 5: 'PlayMusic', 6: 'SearchCreativeWork'}\n{'BookRestaurant': 0, 'SearchScreeningEvent': 1, 'RateBook': 2, 'GetWeather': 3, 'AddToPlaylist': 4, 'PlayMusic': 5, 'SearchCreativeWork': 6}\n","output_type":"stream"}]},{"cell_type":"code","source":"load_data_obj.train_data_frame.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:18:58.496042Z","iopub.execute_input":"2024-04-07T14:18:58.496382Z","iopub.status.idle":"2024-04-07T14:18:58.507239Z","shell.execute_reply.started":"2024-04-07T14:18:58.496326Z","shell.execute_reply":"2024-04-07T14:18:58.506367Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                                                   query              intent  \\\n13253  Find a painting called Beyond the Neighbourhood .  SearchCreativeWork   \n10462                  Play Donald Rubinstein on Pandora           PlayMusic   \n6220     Will it be temperate in Tanana France in a week          GetWeather   \n8219   Put this Jerry Dixon song onto my Tokyo Rising...       AddToPlaylist   \n8561             Add this track to my Rock Hard playlist       AddToPlaylist   \n\n       category  \n13253         6  \n10462         5  \n6220          3  \n8219          4  \n8561          4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n      <th>intent</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13253</th>\n      <td>Find a painting called Beyond the Neighbourhood .</td>\n      <td>SearchCreativeWork</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10462</th>\n      <td>Play Donald Rubinstein on Pandora</td>\n      <td>PlayMusic</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6220</th>\n      <td>Will it be temperate in Tanana France in a week</td>\n      <td>GetWeather</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8219</th>\n      <td>Put this Jerry Dixon song onto my Tokyo Rising...</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8561</th>\n      <td>Add this track to my Rock Hard playlist</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"load_data_obj.train_data_frame","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:18:58.629679Z","iopub.execute_input":"2024-04-07T14:18:58.630029Z","iopub.status.idle":"2024-04-07T14:18:58.643607Z","shell.execute_reply.started":"2024-04-07T14:18:58.629984Z","shell.execute_reply":"2024-04-07T14:18:58.642824Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"                                                   query              intent  \\\n13253  Find a painting called Beyond the Neighbourhood .  SearchCreativeWork   \n10462                  Play Donald Rubinstein on Pandora           PlayMusic   \n6220     Will it be temperate in Tanana France in a week          GetWeather   \n8219   Put this Jerry Dixon song onto my Tokyo Rising...       AddToPlaylist   \n8561             Add this track to my Rock Hard playlist       AddToPlaylist   \n...                                                  ...                 ...   \n711    Book me a reservation for 6 at a highly rated pub      BookRestaurant   \n823    I'd like to eat around ID on august the 16th w...      BookRestaurant   \n1707   Need a table at The Goof in Croatia for a part...      BookRestaurant   \n6082   Tell me the weather forecast for Ethridge , Ma...          GetWeather   \n8891                        Add album to princesas indie       AddToPlaylist   \n\n       category  \n13253         6  \n10462         5  \n6220          3  \n8219          4  \n8561          4  \n...         ...  \n711           0  \n823           0  \n1707          0  \n6082          3  \n8891          4  \n\n[13784 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n      <th>intent</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13253</th>\n      <td>Find a painting called Beyond the Neighbourhood .</td>\n      <td>SearchCreativeWork</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10462</th>\n      <td>Play Donald Rubinstein on Pandora</td>\n      <td>PlayMusic</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6220</th>\n      <td>Will it be temperate in Tanana France in a week</td>\n      <td>GetWeather</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8219</th>\n      <td>Put this Jerry Dixon song onto my Tokyo Rising...</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8561</th>\n      <td>Add this track to my Rock Hard playlist</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>711</th>\n      <td>Book me a reservation for 6 at a highly rated pub</td>\n      <td>BookRestaurant</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>823</th>\n      <td>I'd like to eat around ID on august the 16th w...</td>\n      <td>BookRestaurant</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1707</th>\n      <td>Need a table at The Goof in Croatia for a part...</td>\n      <td>BookRestaurant</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6082</th>\n      <td>Tell me the weather forecast for Ethridge , Ma...</td>\n      <td>GetWeather</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8891</th>\n      <td>Add album to princesas indie</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>13784 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"load_data_obj.validation_data_frame.head().values","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:18:58.767812Z","iopub.execute_input":"2024-04-07T14:18:58.768192Z","iopub.status.idle":"2024-04-07T14:18:58.774472Z","shell.execute_reply.started":"2024-04-07T14:18:58.768133Z","shell.execute_reply":"2024-04-07T14:18:58.773748Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"array([['need a table somewhere in Quarryville 14 hours from now',\n        'BookRestaurant', 0],\n       ['Add a track to playlist Cena con Amigos', 'AddToPlaylist', 4],\n       [\"Please play a sound track from the fifties that's on Iheart\",\n        'PlayMusic', 5],\n       ['what is the forecast in North Carolina for Edgemoor',\n        'GetWeather', 3],\n       ['add Beyond the Valley of 1984 in playlist Folk Music At The Gaslight Cafe',\n        'AddToPlaylist', 4]], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"load_data_obj.train_data_frame.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:18:59.266573Z","iopub.execute_input":"2024-04-07T14:18:59.266991Z","iopub.status.idle":"2024-04-07T14:18:59.278589Z","shell.execute_reply.started":"2024-04-07T14:18:59.266928Z","shell.execute_reply":"2024-04-07T14:18:59.277456Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"                                                   query              intent  \\\n13253  Find a painting called Beyond the Neighbourhood .  SearchCreativeWork   \n10462                  Play Donald Rubinstein on Pandora           PlayMusic   \n6220     Will it be temperate in Tanana France in a week          GetWeather   \n8219   Put this Jerry Dixon song onto my Tokyo Rising...       AddToPlaylist   \n8561             Add this track to my Rock Hard playlist       AddToPlaylist   \n\n       category  \n13253         6  \n10462         5  \n6220          3  \n8219          4  \n8561          4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n      <th>intent</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13253</th>\n      <td>Find a painting called Beyond the Neighbourhood .</td>\n      <td>SearchCreativeWork</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10462</th>\n      <td>Play Donald Rubinstein on Pandora</td>\n      <td>PlayMusic</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6220</th>\n      <td>Will it be temperate in Tanana France in a week</td>\n      <td>GetWeather</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8219</th>\n      <td>Put this Jerry Dixon song onto my Tokyo Rising...</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8561</th>\n      <td>Add this track to my Rock Hard playlist</td>\n      <td>AddToPlaylist</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Initialize the tokenizer\ntokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(load_data_obj.train_data_frame['query'])\n\n# Convert text to sequence of integers\ntrain_sequences = tokenizer.texts_to_sequences(load_data_obj.train_data_frame['query'])\nvalidation_sequences = tokenizer.texts_to_sequences(load_data_obj.validation_data_frame['query'])\n\n# Pad sequences to ensure uniform length\nmax_length = max([len(x) for x in train_sequences])\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\nvalidation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:18:59.864413Z","iopub.execute_input":"2024-04-07T14:18:59.864784Z","iopub.status.idle":"2024-04-07T14:19:00.522608Z","shell.execute_reply.started":"2024-04-07T14:18:59.864731Z","shell.execute_reply":"2024-04-07T14:19:00.521875Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert labels to one-hot encoding\ntrain_labels = to_categorical(load_data_obj.train_data_frame['category'])\nvalidation_labels = to_categorical(load_data_obj.validation_data_frame['category'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:00.524737Z","iopub.execute_input":"2024-04-07T14:19:00.525142Z","iopub.status.idle":"2024-04-07T14:19:00.531216Z","shell.execute_reply.started":"2024-04-07T14:19:00.525081Z","shell.execute_reply":"2024-04-07T14:19:00.530452Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n\n# Assuming max_length is defined here, e.g., max_length = max([len(x) for x in train_sequences])\nmax_length = max([len(x) for x in train_sequences])\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=5000, output_dim=32, input_length=max_length))  # Adjusted output_dim\nmodel.add(Bidirectional(LSTM(128, return_sequences=False)))  # Increased LSTM units and added Bidirectional layer\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))  # Adjusted dropout rate\nmodel.add(Dense(len(train_labels[0]), activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:00.532325Z","iopub.execute_input":"2024-04-07T14:19:00.532601Z","iopub.status.idle":"2024-04-07T14:19:01.020743Z","shell.execute_reply.started":"2024-04-07T14:19:00.532558Z","shell.execute_reply":"2024-04-07T14:19:01.019857Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 35, 32)            160000    \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 256)               164864    \n_________________________________________________________________\ndense_7 (Dense)              (None, 64)                16448     \n_________________________________________________________________\ndropout_119 (Dropout)        (None, 64)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 7)                 455       \n=================================================================\nTotal params: 341,767\nTrainable params: 341,767\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(train_padded, train_labels, epochs=10, validation_data=(validation_padded, validation_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:01.021873Z","iopub.execute_input":"2024-04-07T14:19:01.022145Z","iopub.status.idle":"2024-04-07T14:19:40.297401Z","shell.execute_reply.started":"2024-04-07T14:19:01.022105Z","shell.execute_reply":"2024-04-07T14:19:40.296463Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Train on 13784 samples, validate on 700 samples\nEpoch 1/10\n13784/13784 [==============================] - 6s 458us/sample - loss: 0.5106 - accuracy: 0.8220 - val_loss: 0.1009 - val_accuracy: 0.9686\nEpoch 2/10\n13784/13784 [==============================] - 4s 256us/sample - loss: 0.0783 - accuracy: 0.9772 - val_loss: 0.0855 - val_accuracy: 0.9729\nEpoch 3/10\n13784/13784 [==============================] - 3s 254us/sample - loss: 0.0537 - accuracy: 0.9841 - val_loss: 0.0816 - val_accuracy: 0.9771\nEpoch 4/10\n13784/13784 [==============================] - 4s 261us/sample - loss: 0.0331 - accuracy: 0.9910 - val_loss: 0.0737 - val_accuracy: 0.9700\nEpoch 5/10\n13784/13784 [==============================] - 4s 275us/sample - loss: 0.0241 - accuracy: 0.9932 - val_loss: 0.0719 - val_accuracy: 0.9771\nEpoch 6/10\n13784/13784 [==============================] - 4s 274us/sample - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0780 - val_accuracy: 0.9729\nEpoch 7/10\n13784/13784 [==============================] - 4s 277us/sample - loss: 0.0141 - accuracy: 0.9964 - val_loss: 0.0613 - val_accuracy: 0.9829\nEpoch 8/10\n13784/13784 [==============================] - 4s 270us/sample - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.0882 - val_accuracy: 0.9757\nEpoch 9/10\n13784/13784 [==============================] - 4s 263us/sample - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.0708 - val_accuracy: 0.9829\nEpoch 10/10\n13784/13784 [==============================] - 4s 258us/sample - loss: 0.0149 - accuracy: 0.9959 - val_loss: 0.0816 - val_accuracy: 0.9729\n","output_type":"stream"}]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(validation_padded, validation_labels)\nprint(f'Validation loss: {loss}, Validation accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:40.300416Z","iopub.execute_input":"2024-04-07T14:19:40.300770Z","iopub.status.idle":"2024-04-07T14:19:40.407607Z","shell.execute_reply.started":"2024-04-07T14:19:40.300712Z","shell.execute_reply":"2024-04-07T14:19:40.406729Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"700/700 [==============================] - 0s 110us/sample - loss: 0.0816 - accuracy: 0.9729\nValidation loss: 0.08156434484890529, Validation accuracy: 0.9728571176528931\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Create a DataFrame for results\nresults_df = pd.DataFrame(columns=['model_name', 'validation_accuracy'])\n\n# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'LSTM', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:40.411141Z","iopub.execute_input":"2024-04-07T14:19:40.411448Z","iopub.status.idle":"2024-04-07T14:19:40.425545Z","shell.execute_reply.started":"2024-04-07T14:19:40.411399Z","shell.execute_reply":"2024-04-07T14:19:40.424551Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"  model_name  validation_accuracy\n0       LSTM             0.972857\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RANDOM FOREST\n","metadata":{}},{"cell_type":"code","source":"train_data_frame=load_data_obj.train_data_frame\nvalidation_data_frame=load_data_obj.validation_data_frame\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_data_frame['query'])\nvalidation_tfidf = tfidf_vectorizer.transform(validation_data_frame['query'])\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_labels_encoded = label_encoder.fit_transform(train_data_frame['category'])\nvalidation_labels_encoded = label_encoder.transform(validation_data_frame['category'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:40.427047Z","iopub.execute_input":"2024-04-07T14:19:40.427436Z","iopub.status.idle":"2024-04-07T14:19:40.677903Z","shell.execute_reply.started":"2024-04-07T14:19:40.427379Z","shell.execute_reply":"2024-04-07T14:19:40.677237Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf_classifier.fit(train_tfidf, train_labels_encoded)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:40.679001Z","iopub.execute_input":"2024-04-07T14:19:40.679269Z","iopub.status.idle":"2024-04-07T14:19:46.578125Z","shell.execute_reply.started":"2024-04-07T14:19:40.679231Z","shell.execute_reply":"2024-04-07T14:19:46.577157Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                       warm_start=False)"},"metadata":{}}]},{"cell_type":"code","source":"# Predict on validation set\nvalidation_predictions = rf_classifier.predict(validation_tfidf)\n\n# Calculate accuracy\nvalidation_accuracy = accuracy_score(validation_labels_encoded, validation_predictions)\nprint(f'Validation Accuracy of Random Forest: {validation_accuracy * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:46.579388Z","iopub.execute_input":"2024-04-07T14:19:46.579683Z","iopub.status.idle":"2024-04-07T14:19:46.621547Z","shell.execute_reply.started":"2024-04-07T14:19:46.579633Z","shell.execute_reply":"2024-04-07T14:19:46.620639Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Validation Accuracy of Random Forest: 97.86%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Random Forest', 'validation_accuracy': validation_accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:46.623589Z","iopub.execute_input":"2024-04-07T14:19:46.624011Z","iopub.status.idle":"2024-04-07T14:19:46.633638Z","shell.execute_reply.started":"2024-04-07T14:19:46.623951Z","shell.execute_reply":"2024-04-07T14:19:46.632766Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"      model_name  validation_accuracy\n0           LSTM             0.972857\n1  Random Forest             0.978571\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the Logistic Regression Classifier\nlog_reg_classifier = LogisticRegression(max_iter=1000)  # Increase max_iter if the model doesn't converge\n\n# Train the model\nlog_reg_classifier.fit(train_tfidf, train_labels_encoded)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:46.635065Z","iopub.execute_input":"2024-04-07T14:19:46.635363Z","iopub.status.idle":"2024-04-07T14:19:47.691574Z","shell.execute_reply.started":"2024-04-07T14:19:46.635315Z","shell.execute_reply":"2024-04-07T14:19:47.690672Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"},"metadata":{}}]},{"cell_type":"code","source":"# Predict on validation set\nvalidation_predictions = log_reg_classifier.predict(validation_tfidf)\n\n# Calculate accuracy\nvalidation_accuracy = accuracy_score(validation_labels_encoded, validation_predictions)\nprint(f'Validation Accuracy of Logistic Regression: {validation_accuracy * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:47.693233Z","iopub.execute_input":"2024-04-07T14:19:47.693802Z","iopub.status.idle":"2024-04-07T14:19:47.700720Z","shell.execute_reply.started":"2024-04-07T14:19:47.693602Z","shell.execute_reply":"2024-04-07T14:19:47.699890Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Validation Accuracy of Logistic Regression: 98.29%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Logistic Regression', 'validation_accuracy': validation_accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:47.702220Z","iopub.execute_input":"2024-04-07T14:19:47.703029Z","iopub.status.idle":"2024-04-07T14:19:47.715332Z","shell.execute_reply.started":"2024-04-07T14:19:47.702976Z","shell.execute_reply":"2024-04-07T14:19:47.714360Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=5000, output_dim=16, input_length=max_length))\n\n# First GRU layer with Dropout regularization\nmodel.add(GRU(units=50, return_sequences=True, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Second GRU layer\nmodel.add(GRU(units=50, return_sequences=True, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Third GRU layer\nmodel.add(GRU(units=50, return_sequences=True, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Fourth GRU layer\nmodel.add(GRU(units=50, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Output layer for classification (units = number of classes, softmax activation)\nmodel.add(Dense(units=len(train_labels[0]), activation='softmax'))  # Adjust the units based on the number of classes\n\n# Compile the model for classification\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:47.716586Z","iopub.execute_input":"2024-04-07T14:19:47.717195Z","iopub.status.idle":"2024-04-07T14:19:48.500381Z","shell.execute_reply.started":"2024-04-07T14:19:47.716876Z","shell.execute_reply":"2024-04-07T14:19:48.499252Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 35, 16)            80000     \n_________________________________________________________________\ngru_4 (GRU)                  (None, 35, 50)            10200     \n_________________________________________________________________\ndropout_120 (Dropout)        (None, 35, 50)            0         \n_________________________________________________________________\ngru_5 (GRU)                  (None, 35, 50)            15300     \n_________________________________________________________________\ndropout_121 (Dropout)        (None, 35, 50)            0         \n_________________________________________________________________\ngru_6 (GRU)                  (None, 35, 50)            15300     \n_________________________________________________________________\ndropout_122 (Dropout)        (None, 35, 50)            0         \n_________________________________________________________________\ngru_7 (GRU)                  (None, 50)                15300     \n_________________________________________________________________\ndropout_123 (Dropout)        (None, 50)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 7)                 357       \n=================================================================\nTotal params: 136,457\nTrainable params: 136,457\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_padded, train_labels, epochs=10, validation_data=(validation_padded, validation_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:19:48.501575Z","iopub.execute_input":"2024-04-07T14:19:48.501864Z","iopub.status.idle":"2024-04-07T14:20:49.841000Z","shell.execute_reply.started":"2024-04-07T14:19:48.501823Z","shell.execute_reply":"2024-04-07T14:20:49.840177Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Train on 13784 samples, validate on 700 samples\nEpoch 1/10\n13784/13784 [==============================] - 11s 784us/sample - loss: 1.9474 - accuracy: 0.1411 - val_loss: 1.9464 - val_accuracy: 0.1429\nEpoch 2/10\n13784/13784 [==============================] - 6s 408us/sample - loss: 1.9473 - accuracy: 0.1414 - val_loss: 1.9463 - val_accuracy: 0.1429\nEpoch 3/10\n13784/13784 [==============================] - 6s 413us/sample - loss: 0.8739 - accuracy: 0.6518 - val_loss: 0.2071 - val_accuracy: 0.9471\nEpoch 4/10\n13784/13784 [==============================] - 6s 410us/sample - loss: 0.1849 - accuracy: 0.9529 - val_loss: 0.1493 - val_accuracy: 0.9543\nEpoch 5/10\n13784/13784 [==============================] - 6s 413us/sample - loss: 0.1188 - accuracy: 0.9706 - val_loss: 0.1310 - val_accuracy: 0.9671\nEpoch 6/10\n13784/13784 [==============================] - 6s 408us/sample - loss: 0.0931 - accuracy: 0.9783 - val_loss: 0.1180 - val_accuracy: 0.9743\nEpoch 7/10\n13784/13784 [==============================] - 5s 396us/sample - loss: 0.0744 - accuracy: 0.9831 - val_loss: 0.1580 - val_accuracy: 0.9643\nEpoch 8/10\n13784/13784 [==============================] - 6s 408us/sample - loss: 0.0662 - accuracy: 0.9855 - val_loss: 0.1105 - val_accuracy: 0.9729\nEpoch 9/10\n13784/13784 [==============================] - 6s 400us/sample - loss: 0.0574 - accuracy: 0.9877 - val_loss: 0.1038 - val_accuracy: 0.9700\nEpoch 10/10\n13784/13784 [==============================] - 6s 406us/sample - loss: 0.0535 - accuracy: 0.9888 - val_loss: 0.1968 - val_accuracy: 0.9500\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model\nloss, accuracy = model.evaluate(validation_padded, validation_labels)\nprint(f'Validation loss: {loss}, Validation accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:20:49.842350Z","iopub.execute_input":"2024-04-07T14:20:49.842676Z","iopub.status.idle":"2024-04-07T14:20:49.981920Z","shell.execute_reply.started":"2024-04-07T14:20:49.842615Z","shell.execute_reply":"2024-04-07T14:20:49.981155Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"700/700 [==============================] - 0s 154us/sample - loss: 0.1968 - accuracy: 0.9500\nValidation loss: 0.1967534668317863, Validation accuracy: 0.949999988079071\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'GRU', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:20:49.983188Z","iopub.execute_input":"2024-04-07T14:20:49.983501Z","iopub.status.idle":"2024-04-07T14:20:49.994256Z","shell.execute_reply.started":"2024-04-07T14:20:49.983451Z","shell.execute_reply":"2024-04-07T14:20:49.993279Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RNN ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Embedding, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=5000, output_dim=16, input_length=max_length))\n\n# Add a SimpleRNN layer\nmodel.add(SimpleRNN(units=64, return_sequences=True))\nmodel.add(Dropout(0.2))\n\n# Add another SimpleRNN layer\nmodel.add(SimpleRNN(units=64))\nmodel.add(Dropout(0.2))\n\n# Add the output Dense layer with softmax activation for multi-class classification\nmodel.add(Dense(units=len(train_labels[0]), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:20:49.995854Z","iopub.execute_input":"2024-04-07T14:20:49.996282Z","iopub.status.idle":"2024-04-07T14:20:50.188500Z","shell.execute_reply.started":"2024-04-07T14:20:49.996221Z","shell.execute_reply":"2024-04-07T14:20:50.187470Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, 35, 16)            80000     \n_________________________________________________________________\nsimple_rnn_2 (SimpleRNN)     (None, 35, 64)            5184      \n_________________________________________________________________\ndropout_124 (Dropout)        (None, 35, 64)            0         \n_________________________________________________________________\nsimple_rnn_3 (SimpleRNN)     (None, 64)                8256      \n_________________________________________________________________\ndropout_125 (Dropout)        (None, 64)                0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 7)                 455       \n=================================================================\nTotal params: 93,895\nTrainable params: 93,895\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_padded, train_labels, epochs=10, validation_data=(validation_padded, validation_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:20:50.190427Z","iopub.execute_input":"2024-04-07T14:20:50.190845Z","iopub.status.idle":"2024-04-07T14:23:16.037551Z","shell.execute_reply.started":"2024-04-07T14:20:50.190780Z","shell.execute_reply":"2024-04-07T14:23:16.036732Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Train on 13784 samples, validate on 700 samples\nEpoch 1/10\n13784/13784 [==============================] - 17s 1ms/sample - loss: 0.5194 - accuracy: 0.8371 - val_loss: 0.2853 - val_accuracy: 0.9214\nEpoch 2/10\n13784/13784 [==============================] - 15s 1ms/sample - loss: 0.1414 - accuracy: 0.9594 - val_loss: 0.1566 - val_accuracy: 0.9543\nEpoch 3/10\n13784/13784 [==============================] - 14s 1ms/sample - loss: 0.0779 - accuracy: 0.9774 - val_loss: 0.1649 - val_accuracy: 0.9600\nEpoch 4/10\n13784/13784 [==============================] - 15s 1ms/sample - loss: 0.0594 - accuracy: 0.9834 - val_loss: 0.1892 - val_accuracy: 0.9557\nEpoch 5/10\n13784/13784 [==============================] - 14s 1ms/sample - loss: 0.0582 - accuracy: 0.9828 - val_loss: 0.2602 - val_accuracy: 0.9371\nEpoch 6/10\n13784/13784 [==============================] - 14s 1ms/sample - loss: 0.0464 - accuracy: 0.9861 - val_loss: 0.1732 - val_accuracy: 0.9557\nEpoch 7/10\n13784/13784 [==============================] - 15s 1ms/sample - loss: 0.0486 - accuracy: 0.9845 - val_loss: 0.2192 - val_accuracy: 0.9400\nEpoch 8/10\n13784/13784 [==============================] - 14s 1ms/sample - loss: 0.0456 - accuracy: 0.9867 - val_loss: 0.1921 - val_accuracy: 0.9557\nEpoch 9/10\n13784/13784 [==============================] - 14s 996us/sample - loss: 0.0307 - accuracy: 0.9916 - val_loss: 0.2115 - val_accuracy: 0.9600\nEpoch 10/10\n13784/13784 [==============================] - 14s 1ms/sample - loss: 0.0382 - accuracy: 0.9882 - val_loss: 0.1926 - val_accuracy: 0.9571\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model\nloss, accuracy = model.evaluate(validation_padded, validation_labels)\nprint(f'Validation loss: {loss}, Validation accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:23:16.039494Z","iopub.execute_input":"2024-04-07T14:23:16.039866Z","iopub.status.idle":"2024-04-07T14:23:16.217025Z","shell.execute_reply.started":"2024-04-07T14:23:16.039814Z","shell.execute_reply":"2024-04-07T14:23:16.216238Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"700/700 [==============================] - 0s 207us/sample - loss: 0.1926 - accuracy: 0.9571\nValidation loss: 0.19259030122842108, Validation accuracy: 0.9571428298950195\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'RNN', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:23:16.218300Z","iopub.execute_input":"2024-04-07T14:23:16.218618Z","iopub.status.idle":"2024-04-07T14:23:16.229265Z","shell.execute_reply.started":"2024-04-07T14:23:16.218561Z","shell.execute_reply":"2024-04-07T14:23:16.228218Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import TFBertModel, BertTokenizer\n\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n# Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :])  # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n# Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\nmodel.evaluate(test_dataset.batch(batch_size))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:28:57.392401Z","iopub.execute_input":"2024-04-07T14:28:57.392752Z","iopub.status.idle":"2024-04-07T15:07:32.555225Z","shell.execute_reply.started":"2024-04-07T14:28:57.392710Z","shell.execute_reply":"2024-04-07T15:07:32.554360Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Train for 690 steps\nEpoch 1/10\n690/690 [==============================] - 244s 354ms/step - loss: 1.2345 - accuracy: 0.9401\nEpoch 2/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1788 - accuracy: 0.9872\nEpoch 3/10\n690/690 [==============================] - 228s 331ms/step - loss: 1.1762 - accuracy: 0.9895\nEpoch 4/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1733 - accuracy: 0.9920\nEpoch 5/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1734 - accuracy: 0.9920\nEpoch 6/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1725 - accuracy: 0.9928\nEpoch 7/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1720 - accuracy: 0.9936\nEpoch 8/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1717 - accuracy: 0.9938\nEpoch 9/10\n690/690 [==============================] - 228s 331ms/step - loss: 1.1722 - accuracy: 0.9932\nEpoch 10/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1719 - accuracy: 0.9935\n35/35 [==============================] - 7s 205ms/step - loss: 1.1755 - accuracy: 0.9900\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"[1.1755036558423724, 0.99]"},"metadata":{}}]},{"cell_type":"code","source":"evaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T15:07:32.557236Z","iopub.execute_input":"2024-04-07T15:07:32.557523Z","iopub.status.idle":"2024-04-07T15:07:36.931489Z","shell.execute_reply.started":"2024-04-07T15:07:32.557482Z","shell.execute_reply":"2024-04-07T15:07:36.930492Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"35/35 [==============================] - 4s 124ms/step - loss: 1.1755 - accuracy: 0.9900\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'BERT', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T15:07:36.933030Z","iopub.execute_input":"2024-04-07T15:07:36.933351Z","iopub.status.idle":"2024-04-07T15:07:36.944799Z","shell.execute_reply.started":"2024-04-07T15:07:36.933298Z","shell.execute_reply":"2024-04-07T15:07:36.944045Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n5                 BERT             0.990000\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROBERTA\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import TFRobertaModel, RobertaTokenizer\n\n#Load the tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T15:07:36.946210Z","iopub.execute_input":"2024-04-07T15:07:36.946575Z","iopub.status.idle":"2024-04-07T15:46:38.442743Z","shell.execute_reply.started":"2024-04-07T15:07:36.946522Z","shell.execute_reply":"2024-04-07T15:46:38.441880Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Train for 690 steps\nEpoch 1/10\n690/690 [==============================] - 247s 358ms/step - loss: 1.2303 - accuracy: 0.9393\nEpoch 2/10\n690/690 [==============================] - 231s 335ms/step - loss: 1.1798 - accuracy: 0.9860\nEpoch 3/10\n690/690 [==============================] - 231s 334ms/step - loss: 1.1774 - accuracy: 0.9882\nEpoch 4/10\n690/690 [==============================] - 231s 334ms/step - loss: 1.1769 - accuracy: 0.9885\nEpoch 5/10\n690/690 [==============================] - 231s 334ms/step - loss: 1.1752 - accuracy: 0.9904\nEpoch 6/10\n690/690 [==============================] - 231s 334ms/step - loss: 1.1743 - accuracy: 0.9914\nEpoch 7/10\n690/690 [==============================] - 231s 334ms/step - loss: 1.1753 - accuracy: 0.9901\nEpoch 8/10\n690/690 [==============================] - 231s 335ms/step - loss: 1.1755 - accuracy: 0.9900\nEpoch 9/10\n690/690 [==============================] - 231s 335ms/step - loss: 1.1758 - accuracy: 0.9898\nEpoch 10/10\n690/690 [==============================] - 231s 335ms/step - loss: 1.1745 - accuracy: 0.9909\n35/35 [==============================] - 7s 203ms/step - loss: 1.1740 - accuracy: 0.9914\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Roberta', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T15:46:38.446256Z","iopub.execute_input":"2024-04-07T15:46:38.446613Z","iopub.status.idle":"2024-04-07T15:46:38.458177Z","shell.execute_reply.started":"2024-04-07T15:46:38.446538Z","shell.execute_reply":"2024-04-07T15:46:38.457250Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n5                 BERT             0.990000\n6              Roberta             0.991429\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLnet ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import TFXLNetModel, XLNetTokenizer\n\n# Load the tokenizer and model\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = TFXLNetModel.from_pretrained('xlnet-base-cased')\n\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T15:46:38.460135Z","iopub.execute_input":"2024-04-07T15:46:38.460566Z","iopub.status.idle":"2024-04-07T16:35:43.854210Z","shell.execute_reply.started":"2024-04-07T15:46:38.460502Z","shell.execute_reply":"2024-04-07T16:35:43.853218Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Train for 690 steps\nEpoch 1/10\n690/690 [==============================] - 305s 442ms/step - loss: 1.3841 - accuracy: 0.7779\nEpoch 2/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1936 - accuracy: 0.9718\nEpoch 3/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1864 - accuracy: 0.9787\nEpoch 4/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1855 - accuracy: 0.9801\nEpoch 5/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1819 - accuracy: 0.9835\nEpoch 6/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1804 - accuracy: 0.9851\nEpoch 7/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1792 - accuracy: 0.9861\nEpoch 8/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1803 - accuracy: 0.9852\nEpoch 9/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1784 - accuracy: 0.9871\nEpoch 10/10\n690/690 [==============================] - 292s 423ms/step - loss: 1.1790 - accuracy: 0.9864\n35/35 [==============================] - 7s 206ms/step - loss: 1.1712 - accuracy: 0.9943\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'XLnet', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T16:35:43.855989Z","iopub.execute_input":"2024-04-07T16:35:43.856422Z","iopub.status.idle":"2024-04-07T16:35:43.867823Z","shell.execute_reply.started":"2024-04-07T16:35:43.856357Z","shell.execute_reply":"2024-04-07T16:35:43.866930Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n5                 BERT             0.990000\n6              Roberta             0.991429\n7                XLnet             0.994286\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DistilBert\n","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertModel, DistilBertTokenizer\n\n#Load the tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T16:35:43.869208Z","iopub.execute_input":"2024-04-07T16:35:43.869521Z","iopub.status.idle":"2024-04-07T16:55:54.442008Z","shell.execute_reply.started":"2024-04-07T16:35:43.869471Z","shell.execute_reply":"2024-04-07T16:55:54.441152Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7e43657d4b4eb5b146995a0bb7f6c6"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descriâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ecb96f9448402089bb990a06450af3"}},"metadata":{}},{"name":"stdout","text":"\nTrain for 690 steps\nEpoch 1/10\n690/690 [==============================] - 124s 179ms/step - loss: 1.2423 - accuracy: 0.9335\nEpoch 2/10\n690/690 [==============================] - 116s 169ms/step - loss: 1.1817 - accuracy: 0.9842\nEpoch 3/10\n690/690 [==============================] - 117s 169ms/step - loss: 1.1757 - accuracy: 0.9901\nEpoch 4/10\n690/690 [==============================] - 117s 169ms/step - loss: 1.1743 - accuracy: 0.9912\nEpoch 5/10\n690/690 [==============================] - 117s 169ms/step - loss: 1.1736 - accuracy: 0.9922\nEpoch 6/10\n690/690 [==============================] - 116s 169ms/step - loss: 1.1728 - accuracy: 0.9928\nEpoch 7/10\n690/690 [==============================] - 117s 169ms/step - loss: 1.1720 - accuracy: 0.9933\nEpoch 8/10\n690/690 [==============================] - 116s 169ms/step - loss: 1.1719 - accuracy: 0.9935\nEpoch 9/10\n690/690 [==============================] - 116s 169ms/step - loss: 1.1720 - accuracy: 0.9934\nEpoch 10/10\n690/690 [==============================] - 116s 169ms/step - loss: 1.1715 - accuracy: 0.9941\n35/35 [==============================] - 4s 100ms/step - loss: 1.1767 - accuracy: 0.9886\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'DistilBert', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T16:55:54.443482Z","iopub.execute_input":"2024-04-07T16:55:54.443930Z","iopub.status.idle":"2024-04-07T16:55:54.455512Z","shell.execute_reply.started":"2024-04-07T16:55:54.443866Z","shell.execute_reply":"2024-04-07T16:55:54.454689Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n5                 BERT             0.990000\n6              Roberta             0.991429\n7                XLnet             0.994286\n8           DistilBert             0.988571\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T16:55:54.456818Z","iopub.execute_input":"2024-04-07T16:55:54.457202Z","iopub.status.idle":"2024-04-07T16:55:54.473399Z","shell.execute_reply.started":"2024-04-07T16:55:54.457147Z","shell.execute_reply":"2024-04-07T16:55:54.472656Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n5                 BERT             0.990000\n6              Roberta             0.991429\n7                XLnet             0.994286\n8           DistilBert             0.988571\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Albert","metadata":{}},{"cell_type":"code","source":"from transformers import TFAlbertModel, AlbertTokenizer\n\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nmodel = TFAlbertModel.from_pretrained('albert-base-v2')\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T16:55:54.474744Z","iopub.execute_input":"2024-04-07T16:55:54.475051Z","iopub.status.idle":"2024-04-07T17:32:57.836652Z","shell.execute_reply.started":"2024-04-07T16:55:54.474973Z","shell.execute_reply":"2024-04-07T17:32:57.835934Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descriptiâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f57407e1a6475887195ad416769819"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c91b10c57d48a09321b9ba708bb92c"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=63048440.0, style=ProgressStyle(descripâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8508b6ecaeff4153a88ba1e6c1f9e4a2"}},"metadata":{}},{"name":"stdout","text":"\nTrain for 690 steps\nEpoch 1/10\n690/690 [==============================] - 231s 334ms/step - loss: 1.2816 - accuracy: 0.8855\nEpoch 2/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1920 - accuracy: 0.9738\nEpoch 3/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1867 - accuracy: 0.9791\nEpoch 4/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1869 - accuracy: 0.9787\nEpoch 5/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1849 - accuracy: 0.9806\nEpoch 6/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1846 - accuracy: 0.9807\nEpoch 7/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1801 - accuracy: 0.9853\nEpoch 8/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1788 - accuracy: 0.9867\nEpoch 9/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1800 - accuracy: 0.9854\nEpoch 10/10\n690/690 [==============================] - 219s 317ms/step - loss: 1.1788 - accuracy: 0.9867\n35/35 [==============================] - 8s 216ms/step - loss: 1.1793 - accuracy: 0.9871\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Albert', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T17:32:57.837843Z","iopub.execute_input":"2024-04-07T17:32:57.838122Z","iopub.status.idle":"2024-04-07T17:32:57.848881Z","shell.execute_reply.started":"2024-04-07T17:32:57.838083Z","shell.execute_reply":"2024-04-07T17:32:57.847997Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"            model_name  validation_accuracy\n0                 LSTM             0.972857\n1        Random Forest             0.978571\n2  Logistic Regression             0.982857\n3                  GRU             0.950000\n4                  RNN             0.957143\n5                 BERT             0.990000\n6              Roberta             0.991429\n7                XLnet             0.994286\n8           DistilBert             0.988571\n9               Albert             0.987143\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Electra","metadata":{}},{"cell_type":"code","source":"from transformers import TFElectraModel, ElectraTokenizer\n\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\nmodel = TFElectraModel.from_pretrained('google/electra-base-discriminator')\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T17:32:57.850439Z","iopub.execute_input":"2024-04-07T17:32:57.850856Z","iopub.status.idle":"2024-04-07T18:11:55.565504Z","shell.execute_reply.started":"2024-04-07T17:32:57.850795Z","shell.execute_reply":"2024-04-07T18:11:55.564690Z"},"trusted":true},"execution_count":76,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b76c314687bb49738971681e28261202"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467.0, style=ProgressStyle(description_â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7d5edc7fe74bfca96df2f658c77472"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=438200172.0, style=ProgressStyle(descriâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0035c94ae4114d2792f03ff976af4cc4"}},"metadata":{}},{"name":"stdout","text":"\nTrain for 690 steps\nEpoch 1/10\n690/690 [==============================] - 243s 353ms/step - loss: 1.2771 - accuracy: 0.9069\nEpoch 2/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1831 - accuracy: 0.9836\nEpoch 3/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1796 - accuracy: 0.9860\nEpoch 4/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1765 - accuracy: 0.9893\nEpoch 5/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1779 - accuracy: 0.9877\nEpoch 6/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1760 - accuracy: 0.9896\nEpoch 7/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1750 - accuracy: 0.9905\nEpoch 8/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1744 - accuracy: 0.9910\nEpoch 9/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1747 - accuracy: 0.9907\nEpoch 10/10\n690/690 [==============================] - 228s 330ms/step - loss: 1.1743 - accuracy: 0.9912\n35/35 [==============================] - 7s 200ms/step - loss: 1.1797 - accuracy: 0.9857\n","output_type":"stream"}]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Electra', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T18:11:55.567001Z","iopub.execute_input":"2024-04-07T18:11:55.567393Z","iopub.status.idle":"2024-04-07T18:11:55.579836Z","shell.execute_reply.started":"2024-04-07T18:11:55.567333Z","shell.execute_reply":"2024-04-07T18:11:55.578486Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"             model_name  validation_accuracy\n0                  LSTM             0.972857\n1         Random Forest             0.978571\n2   Logistic Regression             0.982857\n3                   GRU             0.950000\n4                   RNN             0.957143\n5                  BERT             0.990000\n6               Roberta             0.991429\n7                 XLnet             0.994286\n8            DistilBert             0.988571\n9                Albert             0.987143\n10              Electra             0.985714\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Final Evaluation","metadata":{}},{"cell_type":"code","source":"# Find the index of the row with the maximum validation accuracy\nmax_accuracy_index = results_df['validation_accuracy'].idxmax()\n\n# Retrieve the model name with the maximum validation accuracy\nbest_model_name = results_df.loc[max_accuracy_index, 'model_name']\n\nprint(f\"The best model with maximum validation accuracy is: {best_model_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T18:11:55.581109Z","iopub.execute_input":"2024-04-07T18:11:55.581431Z","iopub.status.idle":"2024-04-07T18:11:55.603584Z","shell.execute_reply.started":"2024-04-07T18:11:55.581388Z","shell.execute_reply":"2024-04-07T18:11:55.602562Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"The best model with maximum validation accuracy is: XLnet\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}