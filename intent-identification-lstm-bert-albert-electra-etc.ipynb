{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1119749,"sourceType":"datasetVersion","datasetId":628714}],"dockerImageVersionId":29908,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport os\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Activation, LSTM, SimpleRNN, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"TensorFlow Version:\",tf.__version__)\nprint(\"Hub version: \",hub.__version__)\n# Params for bert model and tokenization\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-07T07:17:46.645776Z","iopub.execute_input":"2024-04-07T07:17:46.646155Z","iopub.status.idle":"2024-04-07T07:17:46.655827Z","shell.execute_reply.started":"2024-04-07T07:17:46.646110Z","shell.execute_reply":"2024-04-07T07:17:46.655049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LoadingData():\n            \n    def __init__(self):\n        train_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Train\")\n        validation_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Validate\")\n        category_id = 0\n        self.cat_to_intent = {}\n        self.intent_to_cat = {}\n        \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                self.cat_to_intent[category_id] = intent_id\n                self.intent_to_cat[intent_id] = category_id\n                category_id+=1\n        print(self.cat_to_intent)\n        print(self.intent_to_cat)\n        '''Training data'''\n        training_data = list() \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                training_data+=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])\n        self.train_data_frame = pd.DataFrame(training_data, columns =['query', 'intent','category'])   \n        \n        self.train_data_frame = self.train_data_frame.sample(frac = 1)\n\n\n        \n        '''Validation data'''\n        validation_data = list()    \n        for dirname, _, filenames in os.walk(validation_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                validation_data +=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])                \n        self.validation_data_frame = pd.DataFrame(validation_data, columns =['query', 'intent','category'])\n\n        self.validation_data_frame = self.validation_data_frame.sample(frac = 1)\n        \n        \n    def make_data_for_intent_from_json(self,json_file,intent_id,cat):\n        json_d = json.load(open(json_file))         \n        \n        json_dict = json_d[intent_id]\n\n        sent_list = list()\n        for i in json_dict:\n            each_list = i['data']\n            sent =\"\"\n            for i in each_list:\n                sent = sent + i['text']+ \" \"\n            sent =sent[:-1]\n            for i in range(3):\n                sent = sent.replace(\"  \",\" \")\n            sent_list.append((sent,intent_id,cat))\n        return sent_list\n            ","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2024-04-07T07:17:47.092394Z","iopub.execute_input":"2024-04-07T07:17:47.092792Z","iopub.status.idle":"2024-04-07T07:17:47.113652Z","shell.execute_reply.started":"2024-04-07T07:17:47.092732Z","shell.execute_reply":"2024-04-07T07:17:47.112798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_data_obj = LoadingData()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:47.638136Z","iopub.execute_input":"2024-04-07T07:17:47.638578Z","iopub.status.idle":"2024-04-07T07:17:48.001177Z","shell.execute_reply.started":"2024-04-07T07:17:47.638505Z","shell.execute_reply":"2024-04-07T07:17:48.000444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_data_obj.train_data_frame.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:48.003025Z","iopub.execute_input":"2024-04-07T07:17:48.003326Z","iopub.status.idle":"2024-04-07T07:17:48.017794Z","shell.execute_reply.started":"2024-04-07T07:17:48.003277Z","shell.execute_reply":"2024-04-07T07:17:48.016836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_data_obj.train_data_frame","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:48.143842Z","iopub.execute_input":"2024-04-07T07:17:48.144180Z","iopub.status.idle":"2024-04-07T07:17:48.157672Z","shell.execute_reply.started":"2024-04-07T07:17:48.144128Z","shell.execute_reply":"2024-04-07T07:17:48.156695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_data_obj.validation_data_frame.head().values","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:48.303305Z","iopub.execute_input":"2024-04-07T07:17:48.303678Z","iopub.status.idle":"2024-04-07T07:17:48.310444Z","shell.execute_reply.started":"2024-04-07T07:17:48.303625Z","shell.execute_reply":"2024-04-07T07:17:48.309485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_data_obj.train_data_frame.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:48.460366Z","iopub.execute_input":"2024-04-07T07:17:48.460747Z","iopub.status.idle":"2024-04-07T07:17:48.471650Z","shell.execute_reply.started":"2024-04-07T07:17:48.460696Z","shell.execute_reply":"2024-04-07T07:17:48.470667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Initialize the tokenizer\ntokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(load_data_obj.train_data_frame['query'])\n\n# Convert text to sequence of integers\ntrain_sequences = tokenizer.texts_to_sequences(load_data_obj.train_data_frame['query'])\nvalidation_sequences = tokenizer.texts_to_sequences(load_data_obj.validation_data_frame['query'])\n\n# Pad sequences to ensure uniform length\nmax_length = max([len(x) for x in train_sequences])\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\nvalidation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:48.950483Z","iopub.execute_input":"2024-04-07T07:17:48.950867Z","iopub.status.idle":"2024-04-07T07:17:49.606748Z","shell.execute_reply.started":"2024-04-07T07:17:48.950809Z","shell.execute_reply":"2024-04-07T07:17:49.605978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert labels to one-hot encoding\ntrain_labels = to_categorical(load_data_obj.train_data_frame['category'])\nvalidation_labels = to_categorical(load_data_obj.validation_data_frame['category'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:49.608482Z","iopub.execute_input":"2024-04-07T07:17:49.608838Z","iopub.status.idle":"2024-04-07T07:17:49.614665Z","shell.execute_reply.started":"2024-04-07T07:17:49.608784Z","shell.execute_reply":"2024-04-07T07:17:49.613678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n\n# Assuming max_length is defined here, e.g., max_length = max([len(x) for x in train_sequences])\nmax_length = max([len(x) for x in train_sequences])\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=5000, output_dim=32, input_length=max_length))  # Adjusted output_dim\nmodel.add(Bidirectional(LSTM(128, return_sequences=False)))  # Increased LSTM units and added Bidirectional layer\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))  # Adjusted dropout rate\nmodel.add(Dense(len(train_labels[0]), activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:49.615985Z","iopub.execute_input":"2024-04-07T07:17:49.616406Z","iopub.status.idle":"2024-04-07T07:17:52.924278Z","shell.execute_reply.started":"2024-04-07T07:17:49.616345Z","shell.execute_reply":"2024-04-07T07:17:52.923481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_padded, train_labels, epochs=10, validation_data=(validation_padded, validation_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:17:52.927322Z","iopub.execute_input":"2024-04-07T07:17:52.927633Z","iopub.status.idle":"2024-04-07T07:18:34.491732Z","shell.execute_reply.started":"2024-04-07T07:17:52.927580Z","shell.execute_reply":"2024-04-07T07:18:34.490915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(validation_padded, validation_labels)\nprint(f'Validation loss: {loss}, Validation accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:34.494742Z","iopub.execute_input":"2024-04-07T07:18:34.495061Z","iopub.status.idle":"2024-04-07T07:18:34.606404Z","shell.execute_reply.started":"2024-04-07T07:18:34.495011Z","shell.execute_reply":"2024-04-07T07:18:34.605658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a DataFrame for results\nresults_df = pd.DataFrame(columns=['model_name', 'validation_accuracy'])\n\n# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'LSTM', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:34.607962Z","iopub.execute_input":"2024-04-07T07:18:34.608286Z","iopub.status.idle":"2024-04-07T07:18:34.623234Z","shell.execute_reply.started":"2024-04-07T07:18:34.608235Z","shell.execute_reply":"2024-04-07T07:18:34.622391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOM FOREST\n","metadata":{}},{"cell_type":"code","source":"train_data_frame=load_data_obj.train_data_frame\nvalidation_data_frame=load_data_obj.validation_data_frame\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_data_frame['query'])\nvalidation_tfidf = tfidf_vectorizer.transform(validation_data_frame['query'])\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_labels_encoded = label_encoder.fit_transform(train_data_frame['category'])\nvalidation_labels_encoded = label_encoder.transform(validation_data_frame['category'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:34.624447Z","iopub.execute_input":"2024-04-07T07:18:34.624770Z","iopub.status.idle":"2024-04-07T07:18:34.925739Z","shell.execute_reply.started":"2024-04-07T07:18:34.624727Z","shell.execute_reply":"2024-04-07T07:18:34.924882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf_classifier.fit(train_tfidf, train_labels_encoded)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:34.926946Z","iopub.execute_input":"2024-04-07T07:18:34.927246Z","iopub.status.idle":"2024-04-07T07:18:41.111136Z","shell.execute_reply.started":"2024-04-07T07:18:34.927204Z","shell.execute_reply":"2024-04-07T07:18:41.110226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on validation set\nvalidation_predictions = rf_classifier.predict(validation_tfidf)\n\n# Calculate accuracy\nvalidation_accuracy = accuracy_score(validation_labels_encoded, validation_predictions)\nprint(f'Validation Accuracy of Random Forest: {validation_accuracy * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:41.112735Z","iopub.execute_input":"2024-04-07T07:18:41.113136Z","iopub.status.idle":"2024-04-07T07:18:41.153271Z","shell.execute_reply.started":"2024-04-07T07:18:41.113076Z","shell.execute_reply":"2024-04-07T07:18:41.152527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Random Forest', 'validation_accuracy': validation_accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:41.154349Z","iopub.execute_input":"2024-04-07T07:18:41.154676Z","iopub.status.idle":"2024-04-07T07:18:41.164621Z","shell.execute_reply.started":"2024-04-07T07:18:41.154621Z","shell.execute_reply":"2024-04-07T07:18:41.163688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the Logistic Regression Classifier\nlog_reg_classifier = LogisticRegression(max_iter=1000)  # Increase max_iter if the model doesn't converge\n\n# Train the model\nlog_reg_classifier.fit(train_tfidf, train_labels_encoded)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:41.165980Z","iopub.execute_input":"2024-04-07T07:18:41.166398Z","iopub.status.idle":"2024-04-07T07:18:42.555039Z","shell.execute_reply.started":"2024-04-07T07:18:41.166338Z","shell.execute_reply":"2024-04-07T07:18:42.554211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on validation set\nvalidation_predictions = log_reg_classifier.predict(validation_tfidf)\n\n# Calculate accuracy\nvalidation_accuracy = accuracy_score(validation_labels_encoded, validation_predictions)\nprint(f'Validation Accuracy of Logistic Regression: {validation_accuracy * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:42.556299Z","iopub.execute_input":"2024-04-07T07:18:42.556818Z","iopub.status.idle":"2024-04-07T07:18:42.563433Z","shell.execute_reply.started":"2024-04-07T07:18:42.556773Z","shell.execute_reply":"2024-04-07T07:18:42.562579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Logistic Regression', 'validation_accuracy': validation_accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:42.564826Z","iopub.execute_input":"2024-04-07T07:18:42.565413Z","iopub.status.idle":"2024-04-07T07:18:42.577713Z","shell.execute_reply.started":"2024-04-07T07:18:42.565319Z","shell.execute_reply":"2024-04-07T07:18:42.576924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=5000, output_dim=16, input_length=max_length))\n\n# First GRU layer with Dropout regularization\nmodel.add(GRU(units=50, return_sequences=True, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Second GRU layer\nmodel.add(GRU(units=50, return_sequences=True, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Third GRU layer\nmodel.add(GRU(units=50, return_sequences=True, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Fourth GRU layer\nmodel.add(GRU(units=50, activation='tanh'))\nmodel.add(Dropout(0.2))\n\n# Output layer for classification (units = number of classes, softmax activation)\nmodel.add(Dense(units=len(train_labels[0]), activation='softmax'))  # Adjust the units based on the number of classes\n\n# Compile the model for classification\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:42.579125Z","iopub.execute_input":"2024-04-07T07:18:42.579635Z","iopub.status.idle":"2024-04-07T07:18:43.379455Z","shell.execute_reply.started":"2024-04-07T07:18:42.579583Z","shell.execute_reply":"2024-04-07T07:18:43.378411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_padded, train_labels, epochs=10, validation_data=(validation_padded, validation_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:43.381582Z","iopub.execute_input":"2024-04-07T07:18:43.381983Z","iopub.status.idle":"2024-04-07T07:19:42.921256Z","shell.execute_reply.started":"2024-04-07T07:18:43.381919Z","shell.execute_reply":"2024-04-07T07:19:42.920316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nloss, accuracy = model.evaluate(validation_padded, validation_labels)\nprint(f'Validation loss: {loss}, Validation accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:19:42.925018Z","iopub.execute_input":"2024-04-07T07:19:42.925333Z","iopub.status.idle":"2024-04-07T07:19:43.064019Z","shell.execute_reply.started":"2024-04-07T07:19:42.925282Z","shell.execute_reply":"2024-04-07T07:19:43.062948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'GRU', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:19:43.065541Z","iopub.execute_input":"2024-04-07T07:19:43.065901Z","iopub.status.idle":"2024-04-07T07:19:43.076785Z","shell.execute_reply.started":"2024-04-07T07:19:43.065853Z","shell.execute_reply":"2024-04-07T07:19:43.075739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Embedding, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Initialize the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=5000, output_dim=16, input_length=max_length))\n\n# Add a SimpleRNN layer\nmodel.add(SimpleRNN(units=64, return_sequences=True))\nmodel.add(Dropout(0.2))\n\n# Add another SimpleRNN layer\nmodel.add(SimpleRNN(units=64))\nmodel.add(Dropout(0.2))\n\n# Add the output Dense layer with softmax activation for multi-class classification\nmodel.add(Dense(units=len(train_labels[0]), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:19:43.078075Z","iopub.execute_input":"2024-04-07T07:19:43.078390Z","iopub.status.idle":"2024-04-07T07:19:43.273985Z","shell.execute_reply.started":"2024-04-07T07:19:43.078346Z","shell.execute_reply":"2024-04-07T07:19:43.273146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_padded, train_labels, epochs=10, validation_data=(validation_padded, validation_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:19:43.275200Z","iopub.execute_input":"2024-04-07T07:19:43.275502Z","iopub.status.idle":"2024-04-07T07:22:02.619627Z","shell.execute_reply.started":"2024-04-07T07:19:43.275453Z","shell.execute_reply":"2024-04-07T07:22:02.618682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nloss, accuracy = model.evaluate(validation_padded, validation_labels)\nprint(f'Validation loss: {loss}, Validation accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:22:02.621328Z","iopub.execute_input":"2024-04-07T07:22:02.621735Z","iopub.status.idle":"2024-04-07T07:22:02.790920Z","shell.execute_reply.started":"2024-04-07T07:22:02.621674Z","shell.execute_reply":"2024-04-07T07:22:02.790032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'RNN', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:22:02.792035Z","iopub.execute_input":"2024-04-07T07:22:02.792317Z","iopub.status.idle":"2024-04-07T07:22:02.802686Z","shell.execute_reply.started":"2024-04-07T07:22:02.792277Z","shell.execute_reply":"2024-04-07T07:22:02.801641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import TFBertModel, BertTokenizer\n\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n# Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :])  # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n# Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\nmodel.evaluate(test_dataset.batch(batch_size))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:22:02.804397Z","iopub.execute_input":"2024-04-07T07:22:02.804849Z","iopub.status.idle":"2024-04-07T08:00:53.346573Z","shell.execute_reply.started":"2024-04-07T07:22:02.804792Z","shell.execute_reply":"2024-04-07T08:00:53.345646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:00:53.347904Z","iopub.execute_input":"2024-04-07T08:00:53.348190Z","iopub.status.idle":"2024-04-07T08:00:57.783893Z","shell.execute_reply.started":"2024-04-07T08:00:53.348148Z","shell.execute_reply":"2024-04-07T08:00:57.783084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'BERT', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:00:57.785423Z","iopub.execute_input":"2024-04-07T08:00:57.785856Z","iopub.status.idle":"2024-04-07T08:00:57.797058Z","shell.execute_reply.started":"2024-04-07T08:00:57.785795Z","shell.execute_reply":"2024-04-07T08:00:57.796086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROBERTA\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import TFRobertaModel, RobertaTokenizer\n\n#Load the tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:00:57.798494Z","iopub.execute_input":"2024-04-07T08:00:57.798952Z","iopub.status.idle":"2024-04-07T08:40:13.350661Z","shell.execute_reply.started":"2024-04-07T08:00:57.798886Z","shell.execute_reply":"2024-04-07T08:40:13.349814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Roberta', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:40:13.352190Z","iopub.execute_input":"2024-04-07T08:40:13.352625Z","iopub.status.idle":"2024-04-07T08:40:13.363816Z","shell.execute_reply.started":"2024-04-07T08:40:13.352534Z","shell.execute_reply":"2024-04-07T08:40:13.362893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLnet ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import TFXLNetModel, XLNetTokenizer\n\n# Load the tokenizer and model\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = TFXLNetModel.from_pretrained('xlnet-base-cased')\n\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:40:13.365443Z","iopub.execute_input":"2024-04-07T08:40:13.365884Z","iopub.status.idle":"2024-04-07T09:29:37.080435Z","shell.execute_reply.started":"2024-04-07T08:40:13.365820Z","shell.execute_reply":"2024-04-07T09:29:37.079658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'XLnet', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:29:37.081837Z","iopub.execute_input":"2024-04-07T09:29:37.082114Z","iopub.status.idle":"2024-04-07T09:29:37.093036Z","shell.execute_reply.started":"2024-04-07T09:29:37.082074Z","shell.execute_reply":"2024-04-07T09:29:37.092229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBert\n","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertModel, DistilBertTokenizer\n\n#Load the tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:29:37.094543Z","iopub.execute_input":"2024-04-07T09:29:37.094931Z","iopub.status.idle":"2024-04-07T09:49:28.803151Z","shell.execute_reply.started":"2024-04-07T09:29:37.094876Z","shell.execute_reply":"2024-04-07T09:49:28.802269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'DistilBert', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:49:28.804476Z","iopub.execute_input":"2024-04-07T09:49:28.804770Z","iopub.status.idle":"2024-04-07T09:49:28.815510Z","shell.execute_reply.started":"2024-04-07T09:49:28.804729Z","shell.execute_reply":"2024-04-07T09:49:28.814610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:06:23.897010Z","iopub.execute_input":"2024-04-07T10:06:23.897391Z","iopub.status.idle":"2024-04-07T10:06:23.904862Z","shell.execute_reply.started":"2024-04-07T10:06:23.897330Z","shell.execute_reply":"2024-04-07T10:06:23.903845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Albert","metadata":{}},{"cell_type":"code","source":"from transformers import TFAlbertModel, AlbertTokenizer\n\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nmodel = TFAlbertModel.from_pretrained('albert-base-v2')\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:10:44.949494Z","iopub.execute_input":"2024-04-07T10:10:44.949872Z","iopub.status.idle":"2024-04-07T10:47:38.842123Z","shell.execute_reply.started":"2024-04-07T10:10:44.949835Z","shell.execute_reply":"2024-04-07T10:47:38.841323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Albert', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:47:38.843923Z","iopub.execute_input":"2024-04-07T10:47:38.844220Z","iopub.status.idle":"2024-04-07T10:47:38.855523Z","shell.execute_reply.started":"2024-04-07T10:47:38.844178Z","shell.execute_reply":"2024-04-07T10:47:38.854470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Electra","metadata":{}},{"cell_type":"code","source":"from transformers import TFElectraModel, ElectraTokenizer\n\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\nmodel = TFElectraModel.from_pretrained('google/electra-base-discriminator')\n\n\n\n# Load and preprocess the data\ndata_train = train_data_frame[['query', 'intent']]\ndata_train['category'] = pd.Categorical(data_train['intent'])\ndata_train['intent'] = data_train['category'].cat.codes\n\n# Load and preprocess the data\ndata_test = validation_data_frame[['query', 'intent']]\ndata_test['category'] = pd.Categorical(data_test['intent'])\ndata_test['intent'] = data_test['category'].cat.codes\n\n\n\n# Extract the training and testing texts and labels\ntrain_texts = data_train['query'].tolist()\ntrain_labels = data_train['intent'].tolist()\ntest_texts = data_test['query'].tolist()\ntest_labels = data_test['intent'].tolist()\n\nmax_length = 128  # Adjust based on your dataset or model's max length\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\ntest_encodings = tokenizer.batch_encode_plus(\n    test_texts, \n    truncation=True, \n    padding='max_length', \n    max_length=max_length, \n    return_tensors=\"tf\",\n    pad_to_max_length=True  # Explicitly enforce padding to max_length\n)\n\n\n\n# Convert the labels to one-hot encoding\nnum_labels = len(data_train['category'].cat.categories)\ntrain_labels_encoded = tf.one_hot(train_labels, num_labels)\ntest_labels_encoded = tf.one_hot(test_labels, num_labels)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n\n#Define the model architecture\ninput_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\noutput = model(input_ids, attention_mask=attention_mask)[0]\noutput = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n#Compile and train the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\n\n# Use smaller batch size\nbatch_size = 20\n\n\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel.fit(train_dataset.batch(batch_size), epochs=10)\n\n# Evaluate the model\n\nevaluation_results = model.evaluate(test_dataset.batch(batch_size))\naccuracy = evaluation_results[1]  # Assuming accuracy is the second metric in the metrics list","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:47:38.857063Z","iopub.execute_input":"2024-04-07T10:47:38.857368Z","iopub.status.idle":"2024-04-07T11:26:18.587517Z","shell.execute_reply.started":"2024-04-07T10:47:38.857324Z","shell.execute_reply":"2024-04-07T11:26:18.586807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert data into the DataFrame\nresults_df = results_df.append({'model_name': 'Electra', 'validation_accuracy': accuracy}, ignore_index=True)\n\n# Display the DataFrame\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:26:18.588784Z","iopub.execute_input":"2024-04-07T11:26:18.589068Z","iopub.status.idle":"2024-04-07T11:26:18.600018Z","shell.execute_reply.started":"2024-04-07T11:26:18.589021Z","shell.execute_reply":"2024-04-07T11:26:18.599195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Evaluation","metadata":{}},{"cell_type":"code","source":"# Find the index of the row with the maximum validation accuracy\nmax_accuracy_index = results_df['validation_accuracy'].idxmax()\n\n# Retrieve the model name with the maximum validation accuracy\nbest_model_name = results_df.loc[max_accuracy_index, 'model_name']\n\nprint(f\"The best model with maximum validation accuracy is: {best_model_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:26:18.602300Z","iopub.execute_input":"2024-04-07T11:26:18.602697Z","iopub.status.idle":"2024-04-07T11:26:18.619468Z","shell.execute_reply.started":"2024-04-07T11:26:18.602643Z","shell.execute_reply":"2024-04-07T11:26:18.618668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:02:22.908027Z","iopub.execute_input":"2024-04-05T14:02:22.908459Z","iopub.status.idle":"2024-04-05T14:02:36.792327Z","shell.execute_reply.started":"2024-04-05T14:02:22.908399Z","shell.execute_reply":"2024-04-05T14:02:36.790888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:01:01.159088Z","iopub.execute_input":"2024-04-05T14:01:01.159645Z","iopub.status.idle":"2024-04-05T14:01:17.213668Z","shell.execute_reply.started":"2024-04-05T14:01:01.159430Z","shell.execute_reply":"2024-04-05T14:01:17.212721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:13:23.669866Z","iopub.execute_input":"2024-04-05T14:13:23.670248Z","iopub.status.idle":"2024-04-05T14:13:33.423417Z","shell.execute_reply.started":"2024-04-05T14:13:23.670196Z","shell.execute_reply":"2024-04-05T14:13:33.422417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:11:53.918344Z","iopub.execute_input":"2024-04-05T14:11:53.918799Z","iopub.status.idle":"2024-04-05T14:12:02.580467Z","shell.execute_reply.started":"2024-04-05T14:11:53.918730Z","shell.execute_reply":"2024-04-05T14:12:02.579486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:05:31.745351Z","iopub.execute_input":"2024-04-05T14:05:31.745730Z","iopub.status.idle":"2024-04-05T14:05:34.553917Z","shell.execute_reply.started":"2024-04-05T14:05:31.745682Z","shell.execute_reply":"2024-04-05T14:05:34.552957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:05:51.458575Z","iopub.execute_input":"2024-04-05T14:05:51.458939Z","iopub.status.idle":"2024-04-05T14:05:54.103478Z","shell.execute_reply.started":"2024-04-05T14:05:51.458890Z","shell.execute_reply":"2024-04-05T14:05:54.102398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:03:08.925714Z","iopub.execute_input":"2024-04-05T14:03:08.926105Z","iopub.status.idle":"2024-04-05T14:03:09.594243Z","shell.execute_reply.started":"2024-04-05T14:03:08.926044Z","shell.execute_reply":"2024-04-05T14:03:09.592791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T13:26:31.299269Z","iopub.execute_input":"2024-04-05T13:26:31.299668Z","iopub.status.idle":"2024-04-05T13:26:31.304902Z","shell.execute_reply.started":"2024-04-05T13:26:31.299609Z","shell.execute_reply":"2024-04-05T13:26:31.304180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}